{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day0810_colab_cifar10_300.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWEN-JUN/keras_/blob/master/day0810_colab_cifar10_300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfdZDmHd1tA3",
        "colab_type": "code",
        "outputId": "aaaeccb3-a825-4d90-d9ea-69317aaf4e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.08,\n",
        "    height_shift_range=0.08,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "\n",
        "X_train, _, Y_train, _ = train_test_split(X_train, Y_train, test_size=0.994)\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 32 ,32, 3)  \n",
        "X_test = X_test.reshape(X_test.shape[0], 32 ,32, 3)\n",
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train)      \n",
        "Y_test = np_utils.to_categorical(Y_test)        \n",
        "\n",
        "\n",
        "IMG_CHANNERLS = 3\n",
        "IMG_ROWS = 32\n",
        "IMG_COLS = 32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train[0])\n",
        "# data_scaling(X_train)\n",
        "# print(X_train[0])\n",
        "# X_train = X_train.reshape(X_train.shape[0], 32 ,32 ,3).astype('float32') / 255\n",
        "# print(X_train[0])\n",
        "\n",
        "def build_network_cnn(keep_prob=0.1, optimizer='adam', conv1 = 20, conv2=30):\n",
        "    \n",
        "    inputs = Input(shape=(32,32,3), name='input')\n",
        "    \n",
        "    x = Conv2D(conv1,(3,3),padding=\"same\",activation=\"relu\", input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNERLS))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(conv1, (3,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=2)(x)\n",
        "    x = Dropout(keep_prob)(x)\n",
        "    \n",
        "    x = Conv2D(conv2,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(conv2, (3,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=2)(x)\n",
        "    x = Dropout(keep_prob)(x)\n",
        "    \n",
        "    \n",
        "#     x = Conv2D(conv3,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Conv2D(conv3, (3,3), activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = MaxPooling2D(pool_size=2)(x)\n",
        "#     x = Dropout(keep_prob)(x)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    prediction = Dense(10, activation='softmax', name='output')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=prediction)\n",
        " \n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',  \n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "    model.fit_generator(data_generator.flow(X_train, Y_train, batch_size=256),\n",
        "                        steps_per_epoch=100,     \n",
        "                        epochs=3,\n",
        "                        verbose=1)  \n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def create_hyperparameters():\n",
        "    batches = [10,20,30,40,50]\n",
        "    optimizers = ['rmsprop', 'adam', 'adadelta']\n",
        "    dropout = np.linspace(0.05,0.5, 10)\n",
        "    epochs = [10,50,100,150, 200]\n",
        "    conv1 = [30,40,50,60,70]\n",
        "    conv2 = [30,40,50,60,70]\n",
        "#     conv3 = [30,40,50,60,70]\n",
        "    return {\"model__batch_size\":batches, \"model__optimizer\":optimizers, \"model__keep_prob\":dropout, \"model__epochs\":epochs,\"model__conv1\":conv1,\"model__conv2\":conv2}      # Map 형태로 반환\n",
        "\n",
        "def data_scaling(x_data):\n",
        "    x_data = x_data.reshape(x_data.shape[0], 32* 32* 3) \n",
        "    MinMaxScaler()\n",
        "    x_data = x_data.reshape(x_data.shape[0], 32, 32, 3)  \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqpxUZzLuO4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = KerasClassifier(build_fn=build_network_cnn, verbose=2)\n",
        "hyperparameters = create_hyperparameters()\n",
        "\n",
        "\n",
        "pipe = Pipeline([('minmax', data_scaling(X_train)), ('model', model)])\n",
        "\n",
        "search = RandomizedSearchCV(estimator=pipe, param_distributions=hyperparameters,\n",
        "                            n_iter=5,  cv=5)\n",
        "\n",
        "\n",
        "\n",
        "search.fit(X_train, Y_train)\n",
        "\n",
        "print(\"score :\", search.best_params_)\n",
        "print(\"score :\", search.best_estimator_)\n",
        "print(\"score :\", search.score(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_OYnLNKzITa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}