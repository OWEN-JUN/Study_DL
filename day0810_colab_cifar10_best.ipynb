{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day0810_colab_cifar10_best.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OWEN-JUN/keras_/blob/master/day0810_colab_cifar10_best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwSCzSuozrGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c231135-f769-43a6-a59e-f818ada1a45d"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.08,\n",
        "    height_shift_range=0.08,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "\n",
        "\n",
        "# X_train, _, Y_train, _ = train_test_split(X_train, Y_train, test_size=0.994)\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 32 ,32, 3)  \n",
        "X_test = X_test.reshape(X_test.shape[0], 32 ,32, 3)\n",
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(Y_train)      \n",
        "Y_test = np_utils.to_categorical(Y_test)        \n",
        "\n",
        "\n",
        "IMG_CHANNERLS = 3\n",
        "IMG_ROWS = 32\n",
        "IMG_COLS = 32\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(X_train[0])\n",
        "# data_scaling(X_train)\n",
        "# print(X_train[0])\n",
        "# X_train = X_train.reshape(X_train.shape[0], 32 ,32 ,3).astype('float32') / 255\n",
        "# print(X_train[0])\n",
        "\n",
        "def build_network_cnn(keep_prob=0.1, optimizer='adam', conv1 = 20, conv2=30):\n",
        "    \n",
        "    inputs = Input(shape=(32,32,3), name='input')\n",
        "    \n",
        "    x = Conv2D(conv1,(3,3),padding=\"same\",activation=\"relu\", input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNERLS))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(conv1, (3,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=2)(x)\n",
        "    x = Dropout(keep_prob)(x)\n",
        "    \n",
        "    x = Conv2D(conv2,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(conv2, (3,3), activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=2)(x)\n",
        "    x = Dropout(keep_prob)(x)\n",
        "    \n",
        "    \n",
        "#     x = Conv2D(conv3,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = Conv2D(conv3, (3,3), activation='relu')(x)\n",
        "#     x = BatchNormalization()(x)\n",
        "#     x = MaxPooling2D(pool_size=2)(x)\n",
        "#     x = Dropout(keep_prob)(x)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    prediction = Dense(10, activation='softmax', name='output')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=prediction)\n",
        " \n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',  \n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "#     model.fit_generator(data_generator.flow(X_train, Y_train, batch_size=256),\n",
        "#                         steps_per_epoch=100,     \n",
        "#                         epochs=3,\n",
        "#                         verbose=1)  \n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def create_hyperparameters():\n",
        "    batches = [10,20,30,40,50]\n",
        "    optimizers = ['rmsprop', 'adam', 'adadelta']\n",
        "    dropout = np.linspace(0.05,0.5, 10)\n",
        "    epochs = [10,50,100,150, 200]\n",
        "    conv1 = [30,40,50,60,70]\n",
        "    conv2 = [30,40,50,60,70]\n",
        "#     conv3 = [30,40,50,60,70]\n",
        "    return {\"model__batch_size\":batches, \"model__optimizer\":optimizers, \"model__keep_prob\":dropout, \"model__epochs\":epochs,\"model__conv1\":conv1,\"model__conv2\":conv2}      # Map 형태로 반환\n",
        "\n",
        "# def data_scaling(x_data):\n",
        "#     x_data = x_data.reshape(x_data.shape[0], 32* 32* 3) \n",
        "#     MinMaxScaler()\n",
        "#     x_data = x_data.reshape(x_data.shape[0], 32, 32, 3)  \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRB9gkLLzyB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# score : {'model__optimizer': 'adadelta', 'model__keep_prob': 0.05, 'model__epochs': 200, 'model__conv2': 60, 'model__conv1': 30, 'model__batch_size': 50}\n",
        "early = EarlyStopping(monitor=\"loss\", patience=10, mode=\"auto\")\n",
        "model = build_network_cnn(keep_prob=0.05, optimizer='adadelta', conv1 = 30, conv2=60)\n",
        "model.fit(X_train,Y_train,batch_size=50, epochs=200, callbacks=[early])\n",
        "\n",
        "print(\"acc:\", model.evaluate(X_test,Y_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPpN-EUJ3oM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a57ac384-1f30-44c0-91df-8f3c78d08ea2"
      },
      "source": [
        "print(\"acc:\", model.evaluate(X_test,Y_test))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 135us/step\n",
            "acc: [1.3444741032123566, 0.7636]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoHcSqpUDlTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}