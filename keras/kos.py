# -*- coding: utf-8 -*-
"""kospi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rNL5z8pF2fJJ5_XIJBpZcbMkznXqMGHF
"""

# from google.colab import drive
# drive.mount('/content/drive/')

# !ls /content/drive/My\ Drive/Colab\ Notebooks

import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import KFold
import numpy as np
from keras.datasets import boston_housing
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, BatchNormalization, Flatten
from sklearn.model_selection import train_test_split

kospi = pd.read_csv("kospi.csv")

kospi

kospi["price_band"] = kospi["day_high"] - kospi["day_low"]
kospi.head()

# Var_Corr = kospi[["volume","price_band"]].corr()
# sns.heatmap(Var_Corr, annot=True)

# plt.figure(figsize=(15,15))
# sns.heatmap(data = kospi.corr(), annot=True, 
# fmt = '.4f', linewidths=.5, cmap='Blues')





kospi=kospi.sort_index(ascending=False)



# stand = StandardScaler()
volume_minmax = MinMaxScaler(feature_range=(0, 3))
won_exchange_minmax = MinMaxScaler(feature_range=(0,5))
# stand.fit(x_train)
# x_train = stand.transform(x_train)
# x_test = stand.transform(x_test)
volume_minmax_list = np.array(kospi["volume"])
volume_minmax_list= volume_minmax_list.reshape((-1,1))
volume_minmax.fit(volume_minmax_list)
volume_minmax_list = volume_minmax.transform(volume_minmax_list)



won_exchange_minmax_list = np.array(kospi["won_exchange"])
won_exchange_minmax_list= won_exchange_minmax_list.reshape((-1,1))
won_exchange_minmax.fit(won_exchange_minmax_list)
won_exchange_minmax_list = won_exchange_minmax.transform(won_exchange_minmax_list)

kospi["volume"]=volume_minmax_list
kospi["won_exchange"]=won_exchange_minmax_list



kospi_label = kospi[["day", "day_end"]]

stand = MinMaxScaler(feature_range=(0, 8))

open_price_list = np.array(kospi["open_price"])
day_high_list = np.array(kospi["day_high"])
day_low_list = np.array(kospi["day_low"])

open_price_list = open_price_list.reshape((-1,1))
day_high_list = day_high_list.reshape((-1,1))
day_low_list = day_low_list.reshape((-1,1))

stand.fit(open_price_list)
open_price_list = stand.transform(open_price_list)
day_high_list = stand.transform(day_high_list)
day_low_list = stand.transform(day_low_list)

kospi["open_price"],kospi["day_high"],kospi["day_low"] = open_price_list, day_high_list, day_low_list



label = np.array(kospi_label)

label

kospi= kospi.drop(["day_end","day"], axis=1)

kospi.head()

band = MinMaxScaler(feature_range=(0, 5))

price_band_list = np.array(kospi["price_band"])
price_band_list = price_band_list.reshape((-1,1))

band.fit(price_band_list)
price_band_list = band.transform(price_band_list)

kospi["price_band"] = price_band_list
kospi = kospi.drop(["open_price"], axis=1)
kospi_list = np.array(kospi)

kospi_list.shape

size = 3

def split_7(seq, size):
    aaa=[]
    for i in range(len(seq)-size+1):
        subset = seq[i:(i+size)]
        aaa.append(subset)

    print(type(aaa))
    return np.array(aaa)

def split_label(seq, size):
    lab = []
    lab = seq[:,-1]
    lab = lab[size:]
    return lab

x_train=split_7(kospi_list, size)
print("==================")
x_train = x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2],1)

x_train.shape
x_pre = x_train[-1]
x_train = x_train[:-1]
print(x_pre)

label.shape
y_train = split_label(label, size)

# print(label[:10])
# print(y_train[:10])
# print(x_train[0])
print(x_train.shape)
print(y_train.shape)
x_pre = x_pre.reshape((1,x_pre.shape[0],1))
print(x_pre.shape)

x_tr, x_t, y_tr, y_t = train_test_split(x_train, y_train, random_state=66, test_size = 0.3)
x_t, x_val, y_t, y_val = train_test_split(x_t, y_t, random_state=66, test_size = 0.5)
###x_train shape (593, 7, 6, 1)
model = Sequential()
model.add(LSTM(10, input_shape=(x_tr.shape[1],1),return_sequences=True))
# model.add(Dropout(0.3))

model.add(Dense(50,activation="relu"))
# model.add(Dropout(0.3))
model.add(Dense(30,activation="relu"))

model.add(Flatten())
model.add(Dense(1))


model.compile(loss = "mse",optimizer="adam", metrics=['accuracy'])
model.fit(x_tr,y_tr,epochs=1500,batch_size=20, verbose=2, validation_data=(x_val, y_val))
loss, acc = model.evaluate(x_t, y_t)
y_ = model.predict(x_t)
print(y_, "ori: ",y_t)
print(loss, acc)
y_pre = model.predict(x_pre)
print(y_pre)

